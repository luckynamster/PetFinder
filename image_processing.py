import numpy as np
from PIL import Image
from io import BytesIO
from sentence_transformers import SentenceTransformer
from pathlib import Path
from typing import List, Tuple


# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å CLIP –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
# –≠—Ç–∞ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
try:
    model = SentenceTransformer('clip-ViT-B-32')
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ CLIP: {str(e)}")
    raise


# =============================================
# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ò –û–ë–†–ê–ë–û–¢–ö–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
# =============================================
def get_image_embedding(image_input) -> np.ndarray:
    try:
        # –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - bytes (BLOB –∏–∑ –ë–î)
        if isinstance(image_input, bytes):
            logger.info("üîç Loading image from BLOB data")
            image = Image.open(BytesIO(image_input))

        # –ï—Å–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ URL
        elif isinstance(image_input, (str, Path)):
            image_path = Path(image_input)
            logger.info(f"üìÇ Loading image: {image_path}")
            if not image_path.exists():
                raise FileNotFoundError(f"Image file not found: {image_path}")
            image = Image.open(image_path)

        # –ï—Å–ª–∏ —É–∂–µ –æ–±—ä–µ–∫—Ç Image
        elif isinstance(image_input, Image.Image):
            image = image_input

        else:
            raise ValueError("‚ùå Unsupported image input type")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB
        if image.mode != 'RGB':
            logger.debug("Converting image to RGB format")
            image = image.convert('RGB')

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä
        logger.info("üñºÔ∏è Processing image with CLIP model...")
        return model.encode(image)

    except Exception as e:
        logger.error(f"üî• Error processing image: {str(e)}")
        raise



# =============================================
# –§–£–ù–ö–¶–ò–Ø –°–†–ê–í–ù–ï–ù–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
# =============================================
def batch_compare(
        source_image: Image.Image,
        image_pairs: List[Tuple[int, Image.Image]]) -> List[Tuple[int, float]]:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å–ø–∏—Å–∫–æ–º (request_id, image)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (request_id, similarity_score)
    """
    results = []

    try:
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        source_embedding = get_image_embedding(source_image)
        source_embedding = source_embedding / np.linalg.norm(source_embedding)

        # –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        for req_id, image in image_pairs:
            try:
                current_embedding = get_image_embedding(image)
                current_embedding = current_embedding / np.linalg.norm(current_embedding)

                similarity = float(np.dot(source_embedding, current_embedding))
                similarity = max(0.0, min(similarity, 1.0))

                results.append((req_id, similarity))

            except Exception as e:
                logger.warning(f"‚è© Skipping request {req_id}: {str(e)}")
                continue

        return sorted(results, key=lambda x: x[1], reverse=True)

    except Exception as e:
        logger.error(f"üî• Batch comparison failed: {str(e)}")
        return []